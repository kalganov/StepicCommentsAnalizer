{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "saved_model = open('models/tweet_model.pickle', 'rb')\n",
    "\n",
    "loaded_classifier = pickle.load(saved_model)\n",
    "\n",
    "saved_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roman_Kalganov\\IdeaProjects\\StepicCommentsAnalizer\\venv_true\\lib\\site-packages\\pandas\\io\\parsers.py:440: ParserWarning: Conflicting values for 'delimiter': ',' was provided, but the dialect specifies '\t'. Using the dialect-specified value.\n  parser = TextFileReader(filepath_or_buffer, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk.classify.util\n",
    "import pandas as pd\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "POSITIVE_COMMENTS_CSV = 'datasets/clean_comments_neg.csv'\n",
    "NEGATIVE_COMMENTS_CSV = 'datasets/clean_comments_pos.csv'\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "tweets_col_number = 6\n",
    "\n",
    "negative_comments = pd.read_csv(\n",
    "    POSITIVE_COMMENTS_CSV, header=None, dialect='excel-tab')[[tweets_col_number]]\n",
    "positive_comments = pd.read_csv(\n",
    "    NEGATIVE_COMMENTS_CSV, header=None, dialect='excel-tab')[[tweets_col_number]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemer = RussianStemmer()\n",
    "regex = re.compile('[^а-яА-Я ]')\n",
    "stem_cache = {}\n",
    "\n",
    "\n",
    "def get_stem(token):\n",
    "    stem = stem_cache.get(token, None)\n",
    "    if stem:\n",
    "        return stem\n",
    "    token = regex.sub('', token).lower()\n",
    "    stem = stemer.stem(token)\n",
    "    stem_cache[token] = stem\n",
    "    return stem\n",
    "\n",
    "\n",
    "stem_count = Counter()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_unique_tokens_in_tweets(tweets):\n",
    "    for _, tweet_series in tweets.iterrows():\n",
    "        tweet = tweet_series[6]\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        for token in tokens:\n",
    "            stem = get_stem(token)\n",
    "            stem_count[stem] += 1\n",
    "\n",
    "\n",
    "count_unique_tokens_in_tweets(negative_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unique_tokens_in_tweets(positive_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['в', 'и', 'не', 'эт', 'на', 'что', 'с', 'а', 'так', 'как', 'для', 'я', 'то', 'есл', 'по', 'но', 'котор', 'курс', 'все', 'из', 'к', 'он', 'у', 'задач', 'был', 'можн', 'вы', 'ил', 'нужн', 'бы', 'прост', 'сам', 'решен', 'за', 'будет', 'ест', 'от', 'же', 'мы', 'тольк', 'ответ', 'дан', 'задан', 'при', 'числ', 'строк', 'очен', 'одн', 'функц', 'может', 'уж', 'перв', 'значен', 'код', 'чтоб', 'над', 'друг', 'ещ', 'больш', 'сво', 'мен', 'вот', 'пот', 'о', 'ег', 'нет', 'раз', 'быт', 'работа', 'мне', 'получ', 'вопрос', 'пример', 'без', 'тест', 'посл', 'поня', 'перемен', 'реш', 'тут', 'когд', 'до', 'правильн', 'чем', 'услов', 'сдела', 'случа', 'спасиб', 'дела', 'тем', 'программ', 'кажд', 'их', 'ваш', 'вам', 'должн', 'хот', 'метод', 'про', 'поч']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(stem_count, key=stem_count.get, reverse=True)[:VOCAB_SIZE]\n",
    "print(vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment: финансовые ресурсы на проект\ncomment_to_feachure: {'contains-word(финансов)': True, 'contains-word(ресурс)': True, 'contains-word(на)': True, 'contains-word(проект)': True}\nчто\n"
     ]
    }
   ],
   "source": [
    "def comment_to_feachure(tweet, show_unknowns=False):\n",
    "    vector = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        stem = get_stem(token)        \n",
    "        if stem:\n",
    "            vector.append(stem)\n",
    "        elif show_unknowns:\n",
    "            print(\"Unknown token: {}\".format(token))\n",
    "    return dict([('contains-word(%s)' % w, True) for w in vector])\n",
    "\n",
    "\n",
    "tweet = negative_comments.iloc[1][6]\n",
    "print(\"comment: {}\".format(tweet))\n",
    "print(\"comment_to_feachure: {}\".format(comment_to_feachure(tweet)))\n",
    "print(vocab[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "\n",
    "neg_comments = []\n",
    "pos_comments = []\n",
    "\n",
    "pos = 6\n",
    "for ii, (_, tweet) in enumerate(negative_comments.iterrows()):\n",
    "    comments.append(tweet[pos])\n",
    "    neg_comments.append((comment_to_feachure(tweet[pos]), 'neg'))\n",
    "for ii, (_, tweet) in enumerate(positive_comments.iterrows()):\n",
    "    comments.append(tweet[pos])\n",
    "    pos_comments.append((comment_to_feachure(tweet[pos]), 'pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negcutoff = len(neg_comments) * 3 / 4\n",
    "poscutoff = len(pos_comments) * 3 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeats = neg_comments[:int(negcutoff)] + pos_comments[:int(poscutoff)]\n",
    "testfeats = neg_comments[int(negcutoff):] + pos_comments[int(poscutoff):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 13953 instances, test on 4651 instances\n"
     ]
    }
   ],
   "source": [
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_classifier = NaiveBayesClassifier.train(trainfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8750806278219738\nMost Informative Features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains-word(медиаграмотн) = True              neg : pos    =    116.5 : 1.0\n   contains-word(послан) = True              neg : pos    =     90.6 : 1.0\ncontains-word(журналистик) = True              neg : pos    =     90.6 : 1.0\n    contains-word(вернм) = True              neg : pos    =     90.6 : 1.0\n     contains-word(нест) = True              neg : pos    =     74.1 : 1.0\n      contains-word(тих) = True              neg : pos    =     56.7 : 1.0\n     contains-word(фигн) = True              neg : pos    =     45.7 : 1.0\n    contains-word(мужик) = True              neg : pos    =     39.7 : 1.0\n   contains-word(диалог) = True              neg : pos    =     35.4 : 1.0\n      contains-word(мим) = True              neg : pos    =     34.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', nltk.classify.util.accuracy(comments_classifier, testfeats))\n",
    "comments_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8176736185766502\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dts_classifier = SklearnClassifier(DecisionTreeClassifier()).train(trainfeats)\n",
    "\n",
    "print('accuracy:', nltk.classify.util.accuracy(dts_classifier, testfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roman_Kalganov\\IdeaProjects\\StepicCommentsAnalizer\\venv_true\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8976564179746291\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SklearnClassifier(SVC()).train(trainfeats)\n",
    "\n",
    "\n",
    "print('accuracy:', nltk.classify.util.accuracy(svm_classifier, testfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.67598365942808\nMost Informative Features\n  contains-word(царевич) = True              pos : neg    =     78.9 : 1.0\n  contains-word(шумахер) = True              neg : pos    =     35.2 : 1.0\n contains-word(калашник) = True              neg : pos    =     28.4 : 1.0\n   contains-word(погибл) = True              neg : pos    =     28.4 : 1.0\n  contains-word(позитив) = True              pos : neg    =     26.8 : 1.0\ncontains-word(соболезнован) = True              neg : pos    =     26.1 : 1.0\n   contains-word(сконча) = True              neg : pos    =     22.2 : 1.0\n   contains-word(сметан) = True              pos : neg    =     21.1 : 1.0\n   contains-word(ублюдк) = True              neg : pos    =     19.5 : 1.0\n  contains-word(почемуу) = True              neg : pos    =     19.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', nltk.classify.util.accuracy(loaded_classifier, testfeats))\n",
    "loaded_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.67598365942808\nMost Informative Features\n  contains-word(царевич) = True              pos : neg    =     78.9 : 1.0\n  contains-word(шумахер) = True              neg : pos    =     35.2 : 1.0\n contains-word(калашник) = True              neg : pos    =     28.4 : 1.0\n   contains-word(погибл) = True              neg : pos    =     28.4 : 1.0\n  contains-word(позитив) = True              pos : neg    =     26.8 : 1.0\ncontains-word(соболезнован) = True              neg : pos    =     26.1 : 1.0\n   contains-word(сконча) = True              neg : pos    =     22.2 : 1.0\n   contains-word(сметан) = True              pos : neg    =     21.1 : 1.0\n   contains-word(ублюдк) = True              neg : pos    =     19.5 : 1.0\n  contains-word(почемуу) = True              neg : pos    =     19.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', nltk.classify.util.accuracy(loaded_classifier, testfeats))\n",
    "loaded_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = open('models/comment_model.pickle', 'wb')\n",
    "\n",
    "pickle.dump(comments_classifier,model)\n",
    "\n",
    "model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'contains-word(финансов)': True,\n  'contains-word(ресурс)': True,\n  'contains-word(на)': True,\n  'contains-word(проект)': True},\n 'neg')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_comments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_classifier.classify(comment_to_feachure('хорошо'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model = open('models/tokenizer_comment.pickle', 'wb')\n",
    "\n",
    "pickle.dump(tokenizer,tokenizer_model)\n",
    "\n",
    "tokenizer_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_model = open('models/stem_cache_comment.pickle', 'wb')\n",
    "\n",
    "pickle.dump(stem_cache,stem_model)\n",
    "\n",
    "stem_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_count_model = open('models/stem_count_comment.pickle', 'wb')\n",
    "\n",
    "pickle.dump(stem_count, stem_count_model)\n",
    "\n",
    "stem_count_model.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy tweet NaiveBayes: 0.7102047294080305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy Stepic NaiveBayes: 0.5124936077165883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy svm: 0.5065862561498174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy dts: 0.5019485443227706\n"
     ]
    }
   ],
   "source": [
    "model_tweet_test = open('models/tweet_model_test_features.pickle', 'rb')\n",
    "\n",
    "test_tweet = pickle.load(model_tweet_test)\n",
    "\n",
    "model_tweet_test.close()\n",
    "\n",
    "print('accuracy tweet NaiveBayes:', nltk.classify.util.accuracy(loaded_classifier, test_tweet))\n",
    "print('accuracy Stepic NaiveBayes:', nltk.classify.util.accuracy(comments_classifier, test_tweet))\n",
    "print('accuracy svm:', nltk.classify.util.accuracy(svm_classifier, test_tweet))\n",
    "print('accuracy dts:', nltk.classify.util.accuracy(dts_classifier, test_tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
